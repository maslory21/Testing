{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maslory21/Testing/blob/main/homeworks/hw_language_modeling_01_simple/hw_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEiYJ3itYv8l"
      },
      "source": [
        "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
        "\n",
        "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "E3nXeT0GYv8l"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import string\n",
        "import os\n",
        "from random import sample\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ena2mCvwYv8m",
        "outputId": "14823db5-bb00-4fde-a7d8-c03db6f8f824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda device is available\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print('{} device is available'.format(device))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPenWOy01Ooa",
        "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
      },
      "source": [
        "#### 1. Загрузка данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIAubZ_8Yv8m",
        "outputId": "3516d000-3026-4536-e580-28b0afcb9f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-03 12:00:45--  https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘onegin.txt’\n",
            "\n",
            "\ronegin.txt            0%[                    ]       0  --.-KB/s               \ronegin.txt          100%[===================>] 256.37K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-12-03 12:00:45 (8.73 MB/s) - ‘onegin.txt’ saved [262521/262521]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
        "\n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "\n",
        "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQYpmGfR_gJ8"
      },
      "source": [
        "#### 2. Построение словаря и предобработка текста\n",
        "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M74-2eT1Yv8n",
        "outputId": "e79196b6-143e-4129-83ac-9bc73fccb056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "tokens = sorted(set(text.lower())) + ['<']\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "assert num_tokens == 84, \"Check the tokenization process\"\n",
        "\n",
        "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
        "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
        "\n",
        "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
        "\n",
        "print(\"Seems fine!\")\n",
        "\n",
        "\n",
        "text_encoded = [token_to_idx[x] for x in text]\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KvJng1_Yv8n"
      },
      "source": [
        "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
        "\n",
        "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
        "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
        "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
        "\n",
        "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
        "\n",
        "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ZuSa31mcYv8n"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "batch_size = 3\n",
        "seq_length = 100\n",
        "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<']\n",
        "\n",
        "def generate_chunk():\n",
        "    global text_encoded, start_column, batch_size, seq_length\n",
        "\n",
        "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1)\n",
        "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
        "    yield np.hstack((start_column, data))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmaB62IzYv8n"
      },
      "source": [
        "Пример батча:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZcYw1UDYv8n",
        "outputId": "8c12f6e7-bb17-4185-edbe-ac7c06b0c030"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[83, 13,  0, 52, 49, 50, 62, 73,  1, 46, 45, 61, 53, 58,  1, 62,\n",
              "        53, 51, 53, 47, 45, 56,  1, 59, 49, 53, 58,  7,  0,  0,  0,  0,\n",
              "        40, 38, 26, 26, 26,  0,  0, 52, 49, 50, 62, 73,  1, 62,  1, 58,\n",
              "        53, 57,  1, 59, 46, 50, 49, 72, 47, 45, 56,  1, 52, 53, 57, 59,\n",
              "        75,  0, 60, 59, 55, 59, 54, 58, 72, 54,  1, 56, 50, 58, 62, 55,\n",
              "        53, 54,  5,  1, 58, 45, 69,  1, 62, 59, 62, 50, 49,  7,  0, 62,\n",
              "        75, 49, 45,  1, 60],\n",
              "       [83, 59, 51, 45, 56, 64, 54, 63, 50,  5,  1, 52, 45,  1, 57, 58,\n",
              "        59, 75,  7,  0, 47, 59, 63,  1, 74, 63, 59,  1, 46, 45, 61, 62,\n",
              "        55, 53, 54,  1, 55, 45, 46, 53, 58, 50, 63, 13,  0, 52, 49, 50,\n",
              "        62, 73,  1, 60, 59, 68, 53, 47, 45, 56,  1, 59, 58,  5,  1, 55,\n",
              "        59, 65, 50, 54,  1, 55, 64, 69, 45, 56,  5,  0, 60, 61, 53, 55,\n",
              "        45, 52, 68, 53, 55, 45,  1, 49, 59, 55, 56, 45, 49, 72,  1, 62,\n",
              "        56, 64, 69, 45, 56],\n",
              "       [83,  0, 53,  1, 55, 58, 53, 51, 55, 64,  1, 60, 59, 64, 63, 61,\n",
              "        64,  1, 68, 53, 63, 45, 56, 81,  0, 53,  1, 62, 63, 45, 61, 72,\n",
              "        54,  1, 46, 45, 61, 53, 58,  1, 52, 49, 50, 62, 73,  1, 51, 53,\n",
              "        47, 45, 56, 13,  0, 62, 59,  1, 57, 58, 59, 54,  5,  1, 46, 72,\n",
              "        47, 45, 56, 59,  5,  1, 47,  1, 47, 59, 62, 55, 61, 50, 62, 50,\n",
              "        58, 73, 50,  5,  0, 52, 49, 50, 62, 73,  1, 60, 59, 49,  1, 59,\n",
              "        55, 58, 59, 57,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "next(generate_chunk())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwSjST8HYv8o"
      },
      "source": [
        "Далее вам предстоит написать код для обучения модели и генерации текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6PoZ2moYv8o",
        "outputId": "2df58318-fa94-44da-dd1c-9fe789d0c56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "тию она.\n",
            "\n",
            "\n",
            "\n",
            "xxxii\n",
            "\n",
            "привычка усладила горе,\n",
            "не отразимое ничем;\n",
            "открытие большое вскоре\n",
            "ее утешило совсем:\n",
            "она меж делом и досугом\n",
            "открыла тайну, как супругом\n",
            "самодержавно управлять,\n",
            "и всё тогда пошло н\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "chunk_len = 200\n",
        "file_len = len(text)\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len - 1)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return text[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solo_chunk = random_chunk()\n",
        "print(solo_chunk)\n",
        "print(type(solo_chunk), len(solo_chunk))\n",
        "\n",
        "print(''.join([idx_to_token[x] for x in solo_chunk]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eUX56UFiOdg",
        "outputId": "37fc9f2e-b11e-48d5-d487-44f9c03a6905"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56 73 58 72  5  1 63 45 55  1 63 59 68 58 72  5  0 63 45 55  1 58 50 60\n",
            " 61 53 62 63 64 60 58 72  1 49 56 76  1 57 64 51 68 53 58  5  0 68 63 59\n",
            "  1 47 53 49  1 53 66  1 64 51  1 61 59 51 49 45 50 63  1 62 60 56 53 58\n",
            "  7  0  0  0  0 40 28 26 26 26  0  0 53  1 47 72  5  1 55 61 45 62 59 63\n",
            " 55 53  1 57]\n",
            "<class 'numpy.ndarray'> 100\n",
            "льны, так точны,\n",
            "так неприступны для мужчин,\n",
            "что вид их уж рождает сплин.\n",
            "\n",
            "\n",
            "\n",
            "xliii\n",
            "\n",
            "и вы, красотки м\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "metadata": {
        "id": "sZ7Re6suddrE"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string), dtype=torch.long)\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = token_to_idx[string[c]]\n",
        "    return tensor\n",
        "\n",
        "print(char_tensor('abcdefx'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBo2oV6sey3A",
        "outputId": "76700195-7b86-45de-a518-c0a132b0fa37"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 19, 20, 21, 22, 23, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_training_set():\n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "metadata": {
        "id": "fDkupJRsl4ui"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        "\n",
        "decoder = RNN(num_tokens, hidden_size, num_tokens, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ndpfpZnaoIFq"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        output_dist = output_dist / output_dist.sum()  # Normalize\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = idx_to_token[top_i.item()]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)[0]\n",
        "\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "B0xqMP8_nfGu"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n"
      ],
      "metadata": {
        "id": "ROrJjEnbn3jB"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / chunk_len\n"
      ],
      "metadata": {
        "id": "yhh5oUZvoBZe"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "n_epochs = 2000\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss = train(*random_training_set())\n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "        print(evaluate('a', 100), '\\n')\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRofXrFAukoI",
        "outputId": "c279d531-9b5f-4cad-e055-20d3095c7aeb"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0m 13s (100 5%) 2.5620]\n",
            "ar7vdeztootk<b8wщу, серостье!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ii\n",
            "\n",
            "одвее до всез ска соброй вия, ладене, тобной,\n",
            "усезмели страчило \n",
            "\n",
            "[0m 27s (200 10%) 2.4412]\n",
            "apкрудь онят,\n",
            "не но ( селя.\n",
            "оне любрастоту, наше рачисть,\n",
            "с он грудь нечнестветец боеденневою потой\n",
            "к \n",
            "\n",
            "[0m 41s (300 15%) 2.1617]\n",
            "a-xlii\n",
            "\n",
            "до мне сет полно же сло дый свый,\n",
            "не пето бетрет редава, лета, нет души не?»! – клаты\n",
            "блажных \n",
            "\n",
            "[0m 54s (400 20%) 2.1035]\n",
            "astta-es)» – – в том немал;\n",
            "пожедной в томной не не пладной\n",
            "и моей свой с очно летой,\n",
            "лубои, не и лы  \n",
            "\n",
            "[1m 8s (500 25%) 2.1199]\n",
            "adal., пребинит сладцой;\n",
            "заменин о клойного в знык?\n",
            "соумали стрим с ревствоц.\n",
            "он посторуский моли нег \n",
            "\n",
            "[1m 22s (600 30%) 1.9984]\n",
            "aba[z<tos\n",
            "\n",
            "словенный ведном вприсамаланна,\n",
            "гласкою сада…\n",
            "\n",
            "\n",
            "\n",
            "xxix\n",
            "\n",
            "как зни тепет, снуя, сладалиной рап \n",
            "\n",
            "[1m 36s (700 35%) 2.3118]\n",
            "a\n",
            "омном стенье сержалось.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xxxiii\n",
            "\n",
            "и сторое лет!\n",
            "\n",
            "\n",
            "\n",
            "xiii\n",
            "\n",
            "он мой глятьюсь порок.\n",
            "хазал!» ониказать \n",
            "\n",
            "[1m 50s (800 40%) 2.2417]\n",
            "a\n",
            "когда плятил к подени скала\n",
            "на жде вольно он в ной простал я ние всердит,\n",
            "устал он в сместваю, на с \n",
            "\n",
            "[2m 4s (900 45%) 1.9058]\n",
            "a. моей,\n",
            "и наши в мечка летей;\n",
            "на в скунов точ, удил он\n",
            "придоветь уставлив остью:\n",
            "та в бруг развлетни \n",
            "\n",
            "[2m 18s (1000 50%) 2.0358]\n",
            "a\n",
            "и вредотерил на преденским подов,\n",
            "и на друд первой в им скоровоскра.\n",
            "и чать, я вздымат своей,\n",
            "холод \n",
            "\n",
            "[2m 32s (1100 55%) 2.0165]\n",
            "anertt»!. txi\n",
            "\n",
            "поэт, но превы задоск…\n",
            "он по скорадца полученне\n",
            "в сновь томали скорне холовь\n",
            "оболки в  \n",
            "\n",
            "[2m 45s (1200 60%) 1.9579]\n",
            "ar… так соснох не\n",
            "вы невозпок плял о красывых,\n",
            "дольда кары заду любу просты;\n",
            "но на тет я прилет перер \n",
            "\n",
            "[2m 59s (1300 65%) 2.0223]\n",
            "at. sul\n",
            "\n",
            "свой сердце все вады прои,\n",
            "и полой поэтом после,\n",
            "с про не всё тебе горазний,\n",
            "милая: видет гр \n",
            "\n",
            "[3m 13s (1400 70%) 2.2398]\n",
            "a€\n",
            "\n",
            "\n",
            "\n",
            "xv\n",
            "\n",
            "и, с непном темне. такой быть\n",
            "могих любим лукой\n",
            "мечтателиник глуя любей.\n",
            "\n",
            "\n",
            "\n",
            "xxxiv\n",
            "\n",
            "и жизнам \n",
            "\n",
            "[3m 27s (1500 75%) 2.2943]\n",
            "ab778 учеть\n",
            "бовы в увостей вдохнодина,\n",
            "(по в нем для раздебскрый,\n",
            "как же слезадну моло\n",
            "она словость з \n",
            "\n",
            "[3m 41s (1600 80%) 2.0208]\n",
            "an7 madme шлюбли дамный устор\n",
            "без чудать и долго много замень\n",
            "что годи шлагокои, моры шал\n",
            "севатся бар \n",
            "\n",
            "[3m 55s (1700 85%) 1.9218]\n",
            "athме\n",
            "так первым полкратом\n",
            "я любов сон душилась пушим альею него молодный проставный\n",
            "маждать дверов г \n",
            "\n",
            "[4m 9s (1800 90%) 1.8487]\n",
            "a\n",
            "– «бегу болья, как жезмы мигя доволодно,\n",
            "из вот и пнуклови мой вахо отрадих\n",
            "всё жела шутоть вжит,\n",
            "к \n",
            "\n",
            "[4m 23s (1900 95%) 1.9863]\n",
            "at глони;\n",
            "гристя поэт…\n",
            "ах голотной любовь.\n",
            "\n",
            "\n",
            "\n",
            "xxiv\n",
            "\n",
            "ни нятся не полошена;\n",
            "я понименье не предом;\n",
            "как  \n",
            "\n",
            "[4m 37s (2000 100%) 2.0762]\n",
            "abgчесство долго мой востити\n",
            "пувались: вседужденьяненье,\n",
            "и душа вник гллаю всегда на соболала!\n",
            "он лет \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_losses)"
      ],
      "metadata": {
        "id": "nYxmEFx_9R7C",
        "outputId": "74871902-50d2-4c07-e177-d81416480031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.3911907653808595, 2.370727844238281, 2.3391116333007815, 2.3980491333007814, 2.3365175018310547, 2.2979264984130863, 2.4283204040527346, 2.373179794311523, 2.3214608459472656, 2.353507858276367, 2.3010834960937503, 2.2465030975341795, 2.293969985961914, 2.266032913208008, 2.3052601318359374, 2.3078264770507815, 2.3373026580810548, 2.253982604980469, 2.2769207153320314, 2.212102310180664, 2.2082490081787105, 2.2236278228759767, 2.1896818847656254, 2.2148701019287107, 2.233379470825195, 2.6578452758789064, 2.2650634002685543, 2.187889221191406, 2.276487426757812, 2.2364360809326174, 2.163976089477539, 2.153363464355469, 2.157206329345703, 2.276905700683593, 2.222420913696289, 2.2705355987548828, 2.1837689666748044, 2.2094019165039063, 2.1236427612304687, 2.145726058959961, 2.168919616699219, 2.199687088012695, 2.2082254943847657, 2.1354349670410153, 2.179661880493164, 2.2325548248291014, 2.1068873596191406, 2.264266403198242, 2.1578556671142577, 2.2194478302001954, 2.2210864868164064, 2.1291230010986326, 2.1850118255615234, 2.1832774200439453, 2.1887800445556644, 2.1357529907226565, 2.187513442993164, 2.125002532958985, 2.126050811767578, 2.150521942138672, 2.1640614013671877, 2.121658950805664, 2.1142898864746096, 2.1118106536865233, 2.197492492675781, 2.1239574432373045, 2.192651824951172, 2.0480084686279296, 2.313509338378906, 2.187999664306641, 2.1166491546630857, 2.1102516021728515, 2.094369445800781, 2.1312904815673823, 2.1151642150878907, 2.164224166870117, 2.195379318237305, 2.0729971008300785, 2.1315038909912105, 2.192082305908203, 2.1848053588867185, 2.068009780883789, 2.1829344024658206, 2.107126541137695, 2.0953423004150387, 2.073065551757812, 2.170202941894531, 2.069987808227539, 2.0435413970947267, 2.140736282348633, 2.1195813903808594, 2.107148010253906, 2.079612548828125, 2.0751179809570317, 2.1221886596679687, 2.135629928588867, 2.077905288696289, 2.070287445068359, 2.119952865600586, 2.1222052612304685, 2.0059618225097657, 2.0731827087402346, 2.009872833251953, 2.0749245452880865, 2.0987322845458984, 2.0472825622558597, 1.9857523651123046, 2.134266784667969, 2.0491688079833983, 2.0800878601074215, 2.0727791137695313, 2.0820258331298827, 2.0240147552490235, 2.0687564697265626, 2.090640365600586, 2.095868408203125, 2.096753997802734, 2.0971372985839842, 2.0171235809326173, 2.0869949645996093, 2.074095550537109, 2.014960998535156, 2.1205511169433597, 2.05564030456543, 2.071109970092774, 2.0657455902099606, 2.055346084594727, 2.048012725830078, 2.036758865356446, 2.10971257019043, 2.1541527557373046, 2.094391571044922, 2.073499816894531, 2.000391159057617, 2.0313009796142576, 2.0952612304687497, 2.027755874633789, 2.0703103332519532, 2.0674643249511715, 2.0308570556640624, 2.0012418365478513, 2.052490966796875, 1.9865165710449222, 2.085995346069336, 2.0156087646484377, 2.0426702575683597, 2.089175750732422, 2.02078337097168, 2.044307098388672, 2.0539572143554694, 1.9875420837402344, 2.060870697021484, 1.9976248779296877, 2.0340448150634765, 2.084336639404297, 2.045635208129883, 2.037696350097656, 1.9961163330078127, 2.0661897125244137, 2.0561456298828125, 2.022918014526367, 1.9415901794433594, 2.0503077545166013, 2.028021514892578, 2.010593948364258, 1.9803322143554685, 1.9662063598632813, 2.1132326965332036, 2.051849594116211, 2.0747822265625, 2.008302261352539, 2.084302932739258, 2.01134016418457, 2.1166523132324224, 2.033455795288086, 2.0347928009033205, 2.045118316650391, 2.062896255493164, 2.015523544311523, 2.0458124389648438, 2.017432495117187, 2.0360940399169922, 1.9826336517333984, 2.006949172973633, 1.9561417694091794, 2.0255046844482423, 1.9866138610839843, 2.0877543487548826, 2.0339628601074216, 1.9965790252685547, 2.1257755279541017, 1.9947454986572268, 2.0105868072509767, 2.06831184387207, 2.015263381958008, 1.9404106445312503, 1.912307159423828, 1.9665427856445312, 1.9857817230224608, 1.9745132446289062]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uu9mntKQYv8o"
      },
      "outputs": [],
      "source": [
        "def generate_sample(char_rnn, seed_phrase=None, max_length=200, temperature=1.0, device=device):\n",
        "    '''\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    '''\n",
        "\n",
        "    if seed_phrase is not None:\n",
        "        x_sequence = [token_to_idx['<sos>']] + [token_to_idx[token] for token in seed_phrase]\n",
        "    else:\n",
        "        x_sequence = [token_to_idx['<sos>']]\n",
        "\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
        "\n",
        "    #feed the seed phrase, if any\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    return ''.join([tokens[ix] for ix in x_sequence.cpu().data.numpy()[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDFUJKYTYv8o"
      },
      "source": [
        "В качестве иллюстрации ниже доступен график значений функции потерь, построенный в ходе обучения авторской сети (сам код для ее обучения вам и предстоит написать)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H04AJtmNYv8o",
        "outputId": "cb13603f-bcea-4c8d-ebca-da5ef1846283"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJJBI9-UYv8o"
      },
      "source": [
        "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO5FuiyiYv8p"
      },
      "source": [
        "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf9m9k4qYv8p",
        "outputId": "55c69deb-43d4-49f8-b778-28cc4f8f923b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<sos> мой дядя самых честных правилас;\n",
            "\n",
            "\n",
            "\n",
            "xiv\n",
            "\n",
            "но как потокой.\n",
            "\n",
            "\n",
            "\n",
            "xii\n",
            "\n",
            "«я свобред не словавран в скорей,\n",
            "для с посвялесь мне моловой,\n",
            "те ты,\n",
            "перегиной в тям праздной\n",
            "и привезут перваю вся вновся сквозь ти стала сблился,\n",
            "и старый свимарной таня обратель любова не когда и нет волностье нежной\n",
            "тишен,\n",
            "перестоком.\n",
            "«поже постаничив очествы\n",
            "в и старько забаньем и заковенью,\n",
            "ее своя моднать наводушта;\n",
            "какой нет поли своем горозный и быле и, законно он ходушних недважный плая\n",
            "с за стра.\n",
            "\n",
            "\n",
            "\n",
            "xvii\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xxvi\n",
            "\n",
            "все \n"
          ]
        }
      ],
      "source": [
        "print(generate_sample(model, ' мой дядя самых честных правил', max_length=500, temperature=0.8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaeVJ1MYYv8p"
      },
      "source": [
        "### Сдача задания\n",
        "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
        "\n",
        "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7_6gSFFYv8p"
      },
      "outputs": [],
      "source": [
        "seed_phrase = ' мой дядя самых честных правил'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFXEF7W-Yv8p"
      },
      "outputs": [],
      "source": [
        "generated_phrases = # your code here\n",
        "\n",
        "# For example:\n",
        "\n",
        "# generated_phrases = [\n",
        "#     generate_sample(\n",
        "#         model,\n",
        "#         ' мой дядя самых честных правил',\n",
        "#         max_length=500,\n",
        "#         temperature=1.\n",
        "#     ).replace('<sos>', '')\n",
        "#     for _ in range(10)\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "716TLo6AYv8p"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "import json\n",
        "if 'generated_phrases' not in locals():\n",
        "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
        "\n",
        "for phrase in generated_phrases:\n",
        "\n",
        "    if not isinstance(phrase, str):\n",
        "        raise ValueError(\"The generated phrase should be a string\")\n",
        "\n",
        "    if len(phrase) != 500:\n",
        "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
        "\n",
        "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
        "\n",
        "\n",
        "submission_dict = {\n",
        "    'token_to_idx': token_to_idx,\n",
        "    'generated_phrases': generated_phrases\n",
        "}\n",
        "\n",
        "with open('submission_dict.json', 'w') as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print('File saved to `submission_dict.json`')\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka8pKcgZYv8p"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}